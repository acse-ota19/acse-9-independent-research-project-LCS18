{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Laura Su (GitHub: LCS18)\"\"\"\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# from livelossplot import PlotLosses\n",
    "# from pycm import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False  ##uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n",
    "    torch.backends.cudnn.enabled   = False\n",
    "\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda installed! Running on GPU!\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n",
    "    print(\"Cuda installed! Running on GPU!\")\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    print(\"No GPU available!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "GeForce 930MX\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform()\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d()\n",
       "      (relu): ReLU(inplace)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d()\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d()\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d()\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d()\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d()\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d()\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d()\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (3): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign()\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "model_save_name = 'faster-r-cnn-resnet50-fpn-finetuning.pt'\n",
    "\n",
    "# load a model pre-trained on COCO\n",
    "model1 = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# replace the classifier with a new one, that has\n",
    "# num_classes which is user-defined\n",
    "num_classes = 2  # 1 class (person) + background\n",
    "# get number of input features for the classifier\n",
    "in_features = model1.roi_heads.box_predictor.cls_score.in_features\n",
    "# replace the pre-trained head with a new one\n",
    "model1.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "model1.load_state_dict(torch.load(F\"{model_save_name}\"))\n",
    "model1.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the video and doing predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the frames from the video every per_frame frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "filename = 'video.mp4'\n",
    "\n",
    "per_frame = 100\n",
    "\n",
    "# Opens the Video file\n",
    "cap = cv2.VideoCapture(filename)\n",
    "\n",
    "# print(cap.isOpened())\n",
    "\n",
    "frames_91 = []\n",
    "\n",
    "i = 0\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if ret == False:\n",
    "        break\n",
    "    if i%per_frame == 0:\n",
    "        frames_91.append(frame)\n",
    "    i += 1\n",
    " \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17999\n"
     ]
    }
   ],
   "source": [
    "nb_tot_frames_91 = i\n",
    "\n",
    "print(nb_tot_frames_91)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resizing the images and converting them as PIL images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "# resizing the frames of the video\n",
    "for i in range(len(frames_91)):\n",
    "    frames_91[i] = cv2.resize(frames_91[i], (640, 360), interpolation=cv2.INTER_LINEAR)\n",
    "    frames_91[i] = Image.fromarray(frames_91[i]).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keeping a copy of the images, for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# # copy of the images (numpy arrays)\n",
    "\n",
    "# import copy\n",
    "\n",
    "# init_imgs_91 = copy.deepcopy([np.array(img) for img in frames_91])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting all the images with the right type, normalised and storing it as a DataLoader object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'vision/references/detection')\n",
    "\n",
    "import transforms as T\n",
    "import utils\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(frames_91)):\n",
    "    frames_91[i] = T.ToTensor()(frames_91[i], {})\n",
    "\n",
    "dataloader_frames_91 = DataLoader(frames_91)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the model in evaluation mode\n",
    "model1.eval()\n",
    "prediction_try_91 = [] # list to store the predictions\n",
    "\n",
    "for X in dataloader_frames_91:\n",
    "    img, target = X #note: \"img is in list[img] configuration\"\n",
    "#     print(img[0].shape)\n",
    "#     print(img[0].to(device))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction_try_91.append(model1([img[0].to(device)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-max suppression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#non-max suppression, cf pyimageresearch\n",
    "\n",
    "# import the necessary packages\n",
    "import numpy as np\n",
    "    \n",
    "#  Felzenszwalb et al.\n",
    "def non_max_suppression_slow(boxes, overlapThresh):\n",
    "    # if there are no boxes, return an empty list\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    " \n",
    "    # initialize the list of picked indexes\n",
    "    pick = []\n",
    " \n",
    "    # grab the coordinates of the bounding boxes\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 2]\n",
    "    y2 = boxes[:, 3]\n",
    " \n",
    "    # compute the area of the bounding boxes and sort the bounding\n",
    "    # boxes by the bottom-right y-coordinate of the bounding box\n",
    "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    idxs = np.argsort(y2)\n",
    "\n",
    "    \n",
    "    # keep looping while some indexes still remain in the indexes\n",
    "    # list\n",
    "    while len(idxs) > 0:\n",
    "        # grab the last index in the indexes list, add the index\n",
    "        # value to the list of picked indexes, then initialize\n",
    "        # the suppression list (i.e. indexes that will be deleted)\n",
    "        # using the last index\n",
    "        last = len(idxs) - 1\n",
    "        i = idxs[last]\n",
    "        pick.append(i)\n",
    "        suppress = [last]\n",
    "\n",
    "\n",
    "        # loop over all indexes in the indexes list\n",
    "        for pos in range(0, last):\n",
    "            # grab the current index\n",
    "            j = idxs[pos]\n",
    " \n",
    "            # find the largest (x, y) coordinates for the start of\n",
    "            # the bounding box and the smallest (x, y) coordinates\n",
    "            # for the end of the bounding box\n",
    "            xx1 = max(x1[i], x1[j])\n",
    "            yy1 = max(y1[i], y1[j])\n",
    "            xx2 = min(x2[i], x2[j])\n",
    "            yy2 = min(y2[i], y2[j])\n",
    " \n",
    "            # compute the width and height of the bounding box\n",
    "            w = max(0, xx2 - xx1 + 1)\n",
    "            h = max(0, yy2 - yy1 + 1)\n",
    " \n",
    "            # compute the ratio of overlap between the computed\n",
    "            # bounding box and the bounding box in the area list\n",
    "            overlap = float(w * h) / area[j]\n",
    "\n",
    "            # if there is sufficient overlap, suppress the\n",
    "            # current bounding box\n",
    "            if overlap > overlapThresh:\n",
    "                suppress.append(pos)\n",
    " \n",
    "        # delete all indexes from the index list that are in the\n",
    "        # suppression list\n",
    "        idxs = np.delete(idxs, suppress)\n",
    "     \n",
    "    return [int(p) for p in pick]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the number of people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the prediction results to have the final values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [1, 0], [1, 0], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0], [0], [0], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "list_picks_91 = []\n",
    "\n",
    "\n",
    "for image_nb in range(len(prediction_try_91)):\n",
    "    boxes_pred_91 = prediction_try_91[image_nb][0][\"boxes\"][prediction_try_91[image_nb][0][\"scores\"] > 0.90]\n",
    "    list_picks_91.append(non_max_suppression_slow(boxes_pred_91.cpu(), 0.3))\n",
    "\n",
    "print(list_picks_91) # each value is the index of the box kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'boxes': tensor([[ 47.6717, 142.0744, 171.7904, 355.5836]]), 'labels': tensor([1]), 'scores': tensor([0.9993])}, {'boxes': tensor([[282.5958, 243.4024, 370.5231, 360.0000],\n",
      "        [  2.7476, 106.8565,  59.1236, 257.1789]]), 'labels': tensor([1, 1]), 'scores': tensor([0.9993, 0.9994])}, {'boxes': tensor([[286.3220, 215.4302, 369.0444, 360.0000],\n",
      "        [ 67.3949, 154.1140, 188.5014, 360.0000]]), 'labels': tensor([1, 1]), 'scores': tensor([0.9993, 0.9995])}, {'boxes': tensor([[181.5401, 230.0909, 295.4908, 359.8666]]), 'labels': tensor([1]), 'scores': tensor([0.9992])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([[229.7172, 302.0378, 339.1597, 359.7471]]), 'labels': tensor([1]), 'scores': tensor([0.9891])}, {'boxes': tensor([[ 28.1003, 132.5384, 151.2050, 354.5649]]), 'labels': tensor([1]), 'scores': tensor([0.9995])}, {'boxes': tensor([[172.9855,  91.4125, 239.9659, 241.9143]]), 'labels': tensor([1]), 'scores': tensor([0.9992])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}, {'boxes': tensor([], size=(0, 4)), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([])}]\n"
     ]
    }
   ],
   "source": [
    "true_preds_91 = []\n",
    "tmp = {}\n",
    "\n",
    "for image_nb in range(len(prediction_try_91)):\n",
    "    for key in prediction_try_91[image_nb][0].keys():\n",
    "        tmp[key] = prediction_try_91[image_nb][0][key][list_picks_91[image_nb]]\n",
    "\n",
    "    true_preds_91.append(tmp)\n",
    "    tmp = {}\n",
    "        \n",
    "print(true_preds_91)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# getting the number of people (according to what was detected)\n",
    "nb_boxes_91 = [len(picks) for picks in list_picks_91]\n",
    "print(nb_boxes_91)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the corresponding time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "599.93\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import *\n",
    "filename1 = 'video.mp4'\n",
    "\n",
    "clip1 = VideoFileClip(filename1)\n",
    "duration_in_sec1 = clip1.duration\n",
    "\n",
    "print(duration_in_sec1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.333129618312128\n"
     ]
    }
   ],
   "source": [
    "time_per_frame1 = (per_frame * duration_in_sec1) / nb_tot_frames_91\n",
    "print(time_per_frame1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 3.333129618312128, 6.666259236624256, 9.999388854936385, 13.332518473248513, 16.66564809156064, 19.99877770987277, 23.331907328184897, 26.665036946497025, 29.998166564809154, 33.33129618312128, 36.66442580143341, 39.99755541974554, 43.330685038057666, 46.663814656369794, 49.99694427468192, 53.33007389299405, 56.66320351130618, 59.99633312961831, 63.329462747930435, 66.66259236624256, 69.99572198455469, 73.32885160286682, 76.66198122117895, 79.99511083949108, 83.3282404578032, 86.66137007611533, 89.99449969442746, 93.32762931273959, 96.66075893105172, 99.99388854936385, 103.32701816767597, 106.6601477859881, 109.99327740430023, 113.32640702261236, 116.65953664092449, 119.99266625923661, 123.32579587754874, 126.65892549586087, 129.992055114173, 133.32518473248513, 136.65831435079724, 139.99144396910938, 143.32457358742153, 146.65770320573364, 149.99083282404575, 153.3239624423579, 156.65709206067004, 159.99022167898215, 163.32335129729427, 166.6564809156064, 169.98961053391855, 173.32274015223066, 176.65586977054278, 179.98899938885492, 183.32212900716706, 186.65525862547918, 189.9883882437913, 193.32151786210343, 196.65464748041558, 199.9877770987277, 203.3209067170398, 206.65403633535195, 209.9871659536641, 213.3202955719762, 216.65342519028832, 219.98655480860046, 223.3196844269126, 226.65281404522472, 229.98594366353683, 233.31907328184897, 236.65220290016111, 239.98533251847323, 243.31846213678534, 246.65159175509748, 249.98472137340963, 253.31785099172174, 256.65098061003386, 259.984110228346, 263.31723984665814, 266.65036946497025, 269.98349908328237, 273.3166287015945, 276.64975831990665, 279.98288793821877, 283.3160175565309, 286.64914717484305, 289.98227679315517, 293.3154064114673, 296.6485360297794, 299.9816656480915, 303.3147952664037, 306.6479248847158, 309.9810545030279, 313.3141841213401, 316.6473137396522, 319.9804433579643, 323.3135729762764, 326.64670259458853, 329.9798322129007, 333.3129618312128, 336.64609144952493, 339.9792210678371, 343.3123506861492, 346.64548030446133, 349.97860992277344, 353.31173954108556, 356.64486915939773, 359.97799877770984, 363.31112839602196, 366.6442580143341, 369.97738763264624, 373.31051725095836, 376.64364686927047, 379.9767764875826, 383.30990610589475, 386.64303572420687, 389.976165342519, 393.30929496083115, 396.64242457914327, 399.9755541974554, 403.3086838157675, 406.6418134340796, 409.9749430523918, 413.3080726707039, 416.641202289016, 419.9743319073282, 423.3074615256403, 426.6405911439524, 429.9737207622645, 433.30685038057663, 436.6399799988888, 439.9731096172009, 443.30623923551303, 446.6393688538252, 449.9724984721373, 453.30562809044943, 456.63875770876155, 459.97188732707366, 463.30501694538583, 466.63814656369794, 469.97127618201006, 473.30440580032223, 476.63753541863434, 479.97066503694646, 483.30379465525857, 486.6369242735707, 489.97005389188286, 493.30318351019497, 496.6363131285071, 499.96944274681925, 503.30257236513137, 506.6357019834435, 509.9688316017556, 513.3019612200677, 516.6350908383798, 519.968220456692, 523.3013500750042, 526.6344796933163, 529.9676093116284, 533.3007389299405, 536.6338685482526, 539.9669981665647, 543.3001277848768, 546.633257403189, 549.9663870215012, 553.2995166398133, 556.6326462581254, 559.9657758764375, 563.2989054947496, 566.6320351130618, 569.9651647313739, 573.2982943496861, 576.6314239679982, 579.9645535863103, 583.2976832046224, 586.6308128229346, 589.9639424412467, 593.2970720595588, 596.6302016778709]\n"
     ]
    }
   ],
   "source": [
    "time_1 = []\n",
    "for k in range(len(frames_91)):\n",
    "    time_1.append(k * time_per_frame1)\n",
    "    \n",
    "print(time_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Checking that the number of boxes and the different times lists have the same size\n",
    "# print(len(nb_boxes_91))\n",
    "# print(len(time_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting number of people vs time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x18581ef3a20>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(9, 5))\n",
    "plt.plot(time_1, nb_boxes_91, \"b*-\", label = \"number of people depending on the time\")\n",
    "\n",
    "plt.title(\"Number of people vs time\")\n",
    "plt.xlabel(\"time (s)\")\n",
    "plt.ylabel(\"Number of people\")\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
